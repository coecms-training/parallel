{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worldwide-democrat",
   "metadata": {},
   "source": [
    "# Types of parallelism\n",
    "\n",
    "## Embarissingly Parallel\n",
    "\n",
    "The simplest method of parallelisation is _'embarissingly parallel'_ - this is a type of parallelisation that requires no extra effort.\n",
    "\n",
    "Consider a program that takes in hourly data, producing daily means. Each day's result is totally independent and requires no knowledge of what happens on the previous or subsequent day. If our input file is split up, say into one file per month, we can process each file independently, say with a loop:\n",
    "\n",
    "```bash\n",
    "for input in tas_*.nc; do\n",
    "    output=tas_daily_${input#*_} # Changes tas_201001.nc to tas_daily_201001.nc\n",
    "    \n",
    "    ./hourly_to_daily.py $input $output\n",
    "done\n",
    "```\n",
    "\n",
    "### GNU Parallel\n",
    "\n",
    "To parallelise this loop automatically you can use [GNU parallel](https://www.gnu.org/software/parallel/) (module 'parallel' at NCI):\n",
    "```bash\n",
    "parallel --jobs 4 ./hourly_to_daily.py ::: tas_*.nc\n",
    "```\n",
    "This will run `./hourly_to_daily.py` on each of the files `tas_*.nc`, with at most 4 jobs running in parallel.\n",
    "\n",
    "There's a couple things to note - first we can't do the fancy output file naming trick, the script itself would need to handle that. Secondly `parallel` can only use a single node if you're using a supercomputer.\n",
    "\n",
    "### Looped Qsub\n",
    "\n",
    "If you have access to a supercomputer you can also try submitting multiple jobs to its queue, using an environment variable to identify which file to process. Using Gadi's qsub:\n",
    "```bash\n",
    "for input in tas_*.nc; do\n",
    "    qsub -v input hourly_to_daily.pbs\n",
    "done\n",
    "```\n",
    "\n",
    "with the job script\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# hourly_to_daily.pbs\n",
    "#PBS -l ncpus=1,walltime=0:10:00,mem=4gb,wd\n",
    "\n",
    "set -eu\n",
    "output=tas_daily_${input#*_} # Changes tas_201001.nc to tas_daily_201001.nc\n",
    "\n",
    "./hourly_to_daily.py $input $output\n",
    "```\n",
    "\n",
    "You can also combine this with `parallel`, say by submitting a year at a time\n",
    "```bash\n",
    "for year in {1990..2010}\n",
    "    qsub -v year hourly_to_daily_year.pbs\n",
    "done\n",
    "```\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# hourly_to_daily_year.pbs\n",
    "#PBS -l ncpus=4,walltime=0:10:00,mem=16gb,wd\n",
    "\n",
    "set -eu\n",
    "module load parallel\n",
    "\n",
    "parallel --jobs $PBS_NCPUS ./hourly_to_daily.py ::: tas_${year}*.nc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-yesterday",
   "metadata": {},
   "source": [
    "## Vectorised operators\n",
    "\n",
    "The next simplest parallelisation method to use is vectorisation. Modern computers have ['vector instruction sets'](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) that can compute multiple values in an array at once.\n",
    "\n",
    "### Fortran / C\n",
    "\n",
    "If you're using Fortran or C, this is for the most part automatic (assuming you're compiling with the `-O2` or `-O3` flags to enable optimisations). It can be helpful to set the `-xHost` (intel) or `-march=native` (gnu) flag too - this lets the compiler use the full vector capability available on the CPU, but you can't then run the program on a different CPU type without recompiling (At NCI, Gadi has different CPU types available in different queues).\n",
    "\n",
    "It is also possible to work with the vector instructions directly, using [Intel MKL](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html) or compiler intrinsic functions. This should only be done by experienced programmers after detailled profiling of the code to ensure you're working on a part of the program that's actually slow.\n",
    "\n",
    "### Python\n",
    "\n",
    "In Python the easiest way to use vectorisation is to use Numpy, or libraries that depend on it (scipy, pandas, xarray etc.). Numpy uses optimised functions when doing array operations.\n",
    "\n",
    ":::{note}\n",
    "As much as possible avoid looping over elements of a numpy array, working on the whole array at once is much faster.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-annual",
   "metadata": {},
   "source": [
    "## Distributed\n",
    "\n",
    "The next step in parallelisation is to use more than one process. While vectorised operations work within a single process, distributed techniques can make use of an arbitrary number of processors, with each running its own program. The individual programs communicate with each other in some way to organise what part of the work each will do.\n",
    "\n",
    "### Fortran / C\n",
    "\n",
    "The standard way to do distributed programming in Fortran and C is with the [MPI (Message Passing Interface)]() library. This is the system that most supercomputer clusters are optimised to use. With MPI you use a special program `mpiexec` to start up a number of copies of your program, then the copies can communicate with each other by passing messages between them.\n",
    "\n",
    "In a climate model each processor might be running a square out of the global grid, with a _'halo'_ of grid cells at the edge being passed to neighboring squares over MPI.\n",
    "\n",
    "```fortran\n",
    "program sendrecv\n",
    "    use mpi_f08\n",
    "    ! Send an array of data from processor 1 to processor 0\n",
    "\n",
    "    real :: a(10)\n",
    "    integer :: rank, tag\n",
    "    \n",
    "    call MPI_Init()\n",
    "    call MPI_Comm_rank(MPI_COMM_WORLD, rank)\n",
    "    tag = 0\n",
    "    \n",
    "    if (rank == 0) then\n",
    "        a = 0\n",
    "        call MPI_Recv(a, size(a), MPI_REAL, 1, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE)\n",
    "        write(*,*) a\n",
    "    else if (rank == 1) then\n",
    "        a = 5\n",
    "        call MPI_Send(a, size(a), MPI_REAL, 0, tag, MPI_COMM_WORLD)\n",
    "    end if\n",
    "    \n",
    "    call MPI_Finalize()\n",
    "end program\n",
    "```\n",
    "\n",
    "### Python\n",
    "\n",
    "Python's [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) library is a built in way to use distributed processing. It starts up extra programs for you when you make a `Pool`, although it can only use the processors on a single node. There are a few different ways to use it, I normally use it to run multiple loop instances in parallel, so rather than\n",
    "```python\n",
    "def process_file(f):\n",
    "    pass # ...\n",
    "\n",
    "for f in files:\n",
    "    process_file(f)\n",
    "```\n",
    "\n",
    "you would use\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(4) as pool:\n",
    "        it = pool.imap_unordered(process_file, files)\n",
    "        for i in it:\n",
    "            pass\n",
    "```\n",
    "\n",
    "Here 4 processors are used to run the function `process_file()` on each element of the array `files`, in an arbitrary order. There are a couple bits of boilerplate - `if __name__ == '__main__':` is needed around the whole script to make sure that child processes start properly, and the loop at the end runs over the function results, making sure that all of the function calls have completed.\n",
    "\n",
    "While `multiprocessing` is good for loops, similar to how you'd run embarassingly parallel code, if you're working with gridded data [dask](https://docs.dask.org/en/latest/) is a good way to parallelise your code. Dask provides replacements for numpy arrays and pandas dataframes which can be split into smaller _'chunks'_, with the chunks then able to be computed in parallel.\n",
    "\n",
    "```python\n",
    "a = dask.array.random.random((10_000, 10_000), chunks=(1000, 1000))\n",
    "mean = a.mean()\n",
    "```\n",
    "\n",
    "For the most part Dask arrays work exactly like their numpy counterparts.\n",
    "\n",
    ":::{note}\n",
    "You can find more information about using Dask in a later chapter.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-edward",
   "metadata": {},
   "source": [
    "## Shared Memory\n",
    "\n",
    "Perhaps the most complex of these options is shared memory parallelisation. In this model, all the processes have a shared view of arrays, and loops can be annotated to run in parallel. This saves on memory, as each process doesn't need its own copy of data, but adds the complexity that writes need to be managed to avoid race conditions. Shared memory will also only work within a single node, if you're wanting to use more than one node you can combine it with message passing so that all the processes on a single node share memory, and the nodes themselves pass messages between each other over MPI. Generally when using shared memory you run a single program that splits into multiple _'threads'_.\n",
    "\n",
    "### Fortran / C\n",
    "\n",
    "The most common library for shared memory parallelisation is [OpenMP](https://www.openmp.org/). This lets you annotate loops with special comments which will then be automatically paralellised. A special compiler flag `-qopenmp` (Intel) or `-openmp` (Gnu) is needed to enable OpenMP comments, and the environment variable `$OMP_NUM_THREADS` needs to be set to the number of processes the program will use.\n",
    "\n",
    "```fortran\n",
    "real :: a(10), b(10), c(10)\n",
    "real :: d\n",
    "integer :: i\n",
    "\n",
    "!$omp parallel loop private(d)\n",
    "do i=1,10\n",
    "    d = a(i) * 10\n",
    "    c(i) = b(i) + d\n",
    "end do\n",
    "```\n",
    "\n",
    "Here all the loop iterations are run in parallel, with each instance running an iteration of the loop. The variable `d` is marked as private - every instance gets its own `d` variable. The arrays are shared, so each instance reads and writes to the same arrays.\n",
    "\n",
    ":::{note}\n",
    "One thing to watch is to make sure each iteration of the loop is independent, as they can be executed in arbitrary order. Here `c(4)` might be evaluated before `c(3)` is, producing unexpected results\n",
    "```fortran\n",
    "c(1) = 1\n",
    "!$omp parallel loop\n",
    "do i=2,10\n",
    "    c(i) = c(i-1) + 1\n",
    "end do\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-inflation",
   "metadata": {},
   "source": [
    "## Design Patterns\n",
    "\n",
    "### Halos\n",
    "\n",
    "### Queues\n",
    "\n",
    "## Common Gotchas\n",
    "\n",
    "### Race Conditions\n",
    "\n",
    "### Locks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-inspector",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3] *",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
