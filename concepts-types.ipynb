{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "split-hazard",
   "metadata": {},
   "source": [
    "# Types of parallelism\n",
    "\n",
    "## Embarissingly Parallel\n",
    "\n",
    "The simplest method of parallelisation is 'embarissingly parallel' - this is a type of parallelisation that requires no extra effort.\n",
    "\n",
    "Consider a program that takes in hourly data, producing daily means. Each day's result is totally independent and requires no knowledge of what happens on the previous or subsequent day. If our input file is split up, say into one file per month, we can process each file independently, say with a loop:\n",
    "\n",
    "```bash\n",
    "for input in tas_*.nc; do\n",
    "    output=tas_daily_${input#*_} # Changes tas_201001.nc to tas_daily_201001.nc\n",
    "    \n",
    "    ./hourly_to_daily.py $input $output\n",
    "done\n",
    "```\n",
    "\n",
    "### GNU Parallel\n",
    "\n",
    "To parallelise this loop automatically you can use [GNU parallel](https://www.gnu.org/software/parallel/) (module 'parallel' at NCI):\n",
    "```bash\n",
    "parallel --jobs 4 ./hourly_to_daily.py ::: tas_*.nc\n",
    "```\n",
    "This will run `./hourly_to_daily.py` on each of the files `tas_*.nc`, with at most 4 jobs running in parallel.\n",
    "\n",
    "There's a couple things to note - first we can't do the fancy output file naming trick, the script itself would need to handle that. Secondly `parallel` can only use a single node if you're using a supercomputer.\n",
    "\n",
    "### Looped Qsub\n",
    "\n",
    "If you have access to a supercomputer you can also try submitting multiple jobs to its queue, using an environment variable to identify which file to process. Using Gadi's qsub:\n",
    "```bash\n",
    "for input in tas_*.nc; do\n",
    "    qsub -v input hourly_to_daily.pbs\n",
    "done\n",
    "```\n",
    "\n",
    "with the job script\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# hourly_to_daily.pbs\n",
    "#PBS -l ncpus=1,walltime=0:10:00,mem=4gb,wd\n",
    "\n",
    "set -eu\n",
    "output=tas_daily_${input#*_} # Changes tas_201001.nc to tas_daily_201001.nc\n",
    "\n",
    "./hourly_to_daily.py $input $output\n",
    "```\n",
    "\n",
    "You can also combine this with `parallel`, say by submitting a year at a time\n",
    "```bash\n",
    "for year in {1990..2010}\n",
    "    qsub -v year hourly_to_daily_year.pbs\n",
    "done\n",
    "```\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# hourly_to_daily_year.pbs\n",
    "#PBS -l ncpus=4,walltime=0:10:00,mem=16gb,wd\n",
    "\n",
    "set -eu\n",
    "module load parallel\n",
    "\n",
    "parallel --jobs $PBS_NCPUS ./hourly_to_daily.py ::: tas_${year}*.nc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-settlement",
   "metadata": {},
   "source": [
    "## Vectorised operators\n",
    "\n",
    "The next simplest parallelisation method to use is vectorisation. Modern computers have ['vector instruction sets'](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) that can compute multiple values in an array at once.\n",
    "\n",
    "### Fortran / C\n",
    "\n",
    "If you're using Fortran or C, this is for the most part automatic (assuming you're compiling with the `-O2` or `-O3` flags to enable optimisations). It can be helpful to set the `-xHost` (intel) or `-march=native` (gnu) flag too - this lets the compiler use the full vector capability available on the CPU, but you can't then run the program on a different CPU type without recompiling (At NCI, Gadi has different CPU types available in different queues).\n",
    "\n",
    "It is also possible to work with the vector instructions directly, using [Intel MKL](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html) or compiler intrinsic functions. This should only be done by experienced programmers after detailled profiling of the code to ensure you're working on a part of the program that's actually slow.\n",
    "\n",
    "### Python\n",
    "\n",
    "In Python the easiest way to use vectorisation is to use Numpy, or libraries that depend on it (scipy, pandas, xarray etc.). Numpy uses optimised functions when doing array operations.\n",
    "\n",
    "```{note}\n",
    "As much as possible avoid looping over elements of a numpy array, working on the whole array at once is much faster.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-mason",
   "metadata": {},
   "source": [
    "## Distributed\n",
    "\n",
    "Fortran:\n",
    "MPI\n",
    "\n",
    "Python:\n",
    "Multiprocessing\n",
    "Dask Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-frequency",
   "metadata": {},
   "source": [
    "## Shared Memory\n",
    "\n",
    "Perhaps the most complex of these options is shared memory parallelisation. In this model, all the processes have a shared view of arrays, and loops can be annotated to run in parallel. This saves on memory, as each process doesn't need its own copy of data, but adds the complexity that writes need to be managed to avoid race conditions. Shared memory will also only work within a single node, if you're wanting to use more than one node you can combine it with message passing so that all the processes on a single node share memory, and the nodes themselves pass messages between each other over MPI.\n",
    "\n",
    "### Fortran / C\n",
    "\n",
    "The most common library for shared memory parallelisation is [OpenMP](https://www.openmp.org/). This lets you annotate loops with special comments which will then be automatically paralellised. A special compiler flag `-qopenmp` (Intel) or `-openmp` (Gnu) is needed to enable OpenMP comments, and the environment variable `$OMP_NUM_THREADS` needs to be set to the number of processes the program will use.\n",
    "\n",
    "```fortran\n",
    "real :: a(10), b(10), c(10)\n",
    "real :: d\n",
    "integer :: i\n",
    "\n",
    "!$omp parallel loop private(d)\n",
    "do i=1,10\n",
    "    d = a(i) * 10\n",
    "    c(i) = b(i) + d\n",
    "end do\n",
    "```\n",
    "\n",
    "Here all the loop iterations are run in parallel, with each instance running an iteration of the loop. The variable `d` is marked as private - every instance gets its own `d` variable. The arrays are shared, so each instance reads and writes to the same arrays.\n",
    "\n",
    ":::{note}\n",
    "One thing to watch is to make sure each iteration of the loop is independent, as they can be executed in arbitrary order. Here `c(4)` might be evaluated before `c(3)` is, producing unexpected results\n",
    "```fortran\n",
    "c(1) = 1\n",
    "!$omp parallel loop\n",
    "do i=2,10\n",
    "    c(i) = c(i-1) + 1\n",
    "end do\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-reservoir",
   "metadata": {},
   "source": [
    "## Design Patterns\n",
    "\n",
    "## Common Gotchas\n",
    "\n",
    "### Race Conditions\n",
    "\n",
    "### Locks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-belfast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3] *",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
